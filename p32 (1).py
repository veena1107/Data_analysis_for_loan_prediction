# -*- coding: utf-8 -*-
"""P32.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mzTrJ14x2DtizIa54VrDoHC9tiOnWxRO
"""

import pandas as pd

"""# New Section"""

df=pd.read_csv("loan-test.csv")
df

df1=pd.read_csv("loan-train.csv")
df1

df.isnull().sum()

df.info()

df1.isnull().sum()

df.head()

df.describe()

df1.head()

df1.info()

df1.describe()

df=df.drop_duplicates()
df

df1=df1.drop_duplicates()
df1

# fill the missing values for numerical terms - mean
df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].mean())
df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mean())
df['Credit_History'] = df['Credit_History'].fillna(df['Credit_History'].mean())

# fill the missing values for categorical terms - mode
df['Gender'] = df["Gender"].fillna(df['Gender'].mode()[0])
df['Married'] = df["Married"].fillna(df['Married'].mode()[0])
df['Dependents'] = df["Dependents"].fillna(df['Dependents'].mode()[0])
df['Self_Employed'] = df["Self_Employed"].fillna(df['Self_Employed'].mode()[0])

df.isnull().sum()

#preprocessing by 23761A5410
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load datasets
print("STEP 1: Loading datasets")
train = pd.read_csv('loan-train.csv')
test = pd.read_csv('loan-test.csv')

print("\nTraining Data (First 5 rows):")
print(train.head())
print("\nTest Data (First 5 rows):")
print(test.head())

print("\nTraining Data Shape:", train.shape)
print("Test Data Shape:", test.shape)
print("\nMissing Values in Training Data:")
print(train.isnull().sum())
print("\nMissing Values in Test Data:")
print(test.isnull().sum())

print("\nSTEP 2: Handling Missing Values")

def handle_missing_data(df, dataset_name):
    print(f"\nProcessing {dataset_name}:")

    # Categorical columns
    cat_cols = ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History']
    for col in cat_cols:
        mode_val = df[col].mode()[0]
        df[col] = df[col].fillna(mode_val)
        print(f"- Filled {col} with mode: {mode_val}")

    # Numerical columns
    num_cols = ['LoanAmount', 'Loan_Amount_Term']
    for col in num_cols:
        median_val = df[col].median()
        df[col] = df[col].fillna(median_val)
        print(f"- Filled {col} with median: {median_val:.2f}")

    # Convert '3+' to 3 in Dependents
    df['Dependents'] = df['Dependents'].replace('3+', 3).astype(float)

    print(f"\n{dataset_name} after handling missing values (First 5 rows):")
    print(df.head())
    print(f"\nMissing values in {dataset_name} after cleaning:")
    print(df.isnull().sum())
    return df

train = handle_missing_data(train, "Training Data")
test = handle_missing_data(test, "Test Data")

print("\nSTEP 3: Feature Engineering")

def create_new_features(df, dataset_name):
    print(f"\nCreating new features for {dataset_name}:")

    # Original features
    print("\nOriginal features:")
    print(df.columns.tolist())

    # New features
    df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']
    df['EMI'] = df['LoanAmount'] / df['Loan_Amount_Term']
    df['Income_to_Loan_Ratio'] = df['TotalIncome'] / df['LoanAmount']
    df['Loan_by_Term'] = df['LoanAmount'] * 1000 / df['Loan_Amount_Term']
    df['LogTotalIncome'] = np.log1p(df['TotalIncome'])
    df['LogLoanAmount'] = np.log1p(df['LoanAmount'])

    print("\nNew features added:")
    new_features = ['TotalIncome', 'EMI', 'Income_to_Loan_Ratio',
                   'Loan_by_Term', 'LogTotalIncome', 'LogLoanAmount']
    print(new_features)

    print(f"\n{dataset_name} with new features (First 5 rows):")
    print(df.head())
    return df

train = create_new_features(train, "Training Data")
test = create_new_features(test, "Test Data")

print("\nSTEP 4: Outlier Treatment")

def treat_outliers(df, dataset_name):
    print(f"\nTreating outliers in {dataset_name}:")

    num_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',
               'TotalIncome', 'EMI', 'Income_to_Loan_Ratio']

    for col in num_cols:
        upper_limit = df[col].quantile(0.99)
        n_outliers = (df[col] > upper_limit).sum()
        df[col] = np.where(df[col] > upper_limit, upper_limit, df[col])
        print(f"- Capped {n_outliers} outliers in {col} at {upper_limit:.2f}")

    print(f"\n{dataset_name} after outlier treatment (First 5 rows):")
    print(df.head())
    return df

train = treat_outliers(train, "Training Data")
test = treat_outliers(test, "Test Data")

print("\nSTEP 5: Categorical Encoding")

def encode_categorical(df, dataset_name, is_train=True):
    print(f"\nEncoding categorical variables in {dataset_name}:")

    # Binary encoding
    binary_mappings = {
        'Gender': {'Male': 1, 'Female': 0},
        'Married': {'Yes': 1, 'No': 0},
        'Education': {'Graduate': 1, 'Not Graduate': 0},
        'Self_Employed': {'Yes': 1, 'No': 0}
    }

    for col, mapping in binary_mappings.items():
        df[col] = df[col].map(mapping)
        print(f"- Encoded {col}: {mapping}")

    # One-hot encode Property_Area
    df = pd.get_dummies(df, columns=['Property_Area'], prefix=['Area'])
    print("\nOne-hot encoded Property_Area columns:")
    print([col for col in df.columns if col.startswith('Area_')])

    # Encode target if training data
    if is_train and 'Loan_Status' in df.columns:
        df['Loan_Status'] = LabelEncoder().fit_transform(df['Loan_Status'])
        print("\nEncoded Loan_Status: Y->1, N->0")

    print(f"\n{dataset_name} after encoding (First 5 rows):")
    print(df.head())
    return df

train = encode_categorical(train, "Training Data")
test = encode_categorical(test, "Test Data", is_train=False)

print("\nSTEP 7: Creating Final DataFrames")

# Define numeric_features and categorical_features
numeric_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term',
                   'Credit_History', 'Dependents', 'TotalIncome', 'EMI',
                   'Income_to_Loan_Ratio', 'Loan_by_Term', 'LogTotalIncome', 'LogLoanAmount']
categorical_features = ['Gender', 'Married', 'Education', 'Self_Employed']

# Get feature names
numeric_features_processed = numeric_features
categorical_features_processed = categorical_features
all_feature_names = numeric_features_processed + categorical_features_processed + ['Area_Rural', 'Area_Semiurban', 'Area_Urban']  # Add one-hot encoded area features

# Extract features and target
X_train = train[all_feature_names]
y_train = train['Loan_Status']
X_test = test[all_feature_names]

# Preprocessing steps were likely missing here.
# Assuming a simple scaling for numerical features:

from sklearn.preprocessing import StandardScaler

# Create a scaler object
scaler = StandardScaler()

# Fit the scaler to the training data and transform both training and test data
X_train_processed = scaler.fit_transform(X_train[numeric_features_processed])
X_test_processed = scaler.transform(X_test[numeric_features_processed])

# Convert to DataFrame
X_train_df = pd.DataFrame(X_train_processed, columns=numeric_features_processed)
X_test_df = pd.DataFrame(X_test_processed, columns=numeric_features_processed)

# Add back the categorical features and one-hot encoded area features
X_train_df = pd.concat([X_train_df, X_train[categorical_features_processed + ['Area_Rural', 'Area_Semiurban', 'Area_Urban']].reset_index(drop=True)], axis=1)
X_test_df = pd.concat([X_test_df, X_test[categorical_features_processed + ['Area_Rural', 'Area_Semiurban', 'Area_Urban']].reset_index(drop=True)], axis=1)

print("\nFinal Training Data Columns:")
print(X_train_df.columns.tolist())
print("\nFinal Test Data Columns:")
print(X_test_df.columns.tolist())

print("\nFinal Training Data (First 5 rows):")
print(X_train_df.head())
print("\nFinal Test Data (First 5 rows):")
print(X_test_df.head())

print("\nPreprocessing complete!")
print("Final Training Data Shape:", X_train_df.shape)
print("Final Test Data Shape:", X_test_df.shape)

#check for missing values
import missingno as msno

# Visualizing missing values
msno.bar(df)  # Bar chart of missing data

plt.figure(figsize=(6,4))
sns.countplot(x='Loan_Status', data=train, palette='pastel')
plt.title('Loan Approval Status Distribution')
plt.show()

distribution of applicants income
plt.figure(figsize=(8,5))
sns.histplot(d#f['ApplicantIncome'], kde=True, bins=30, color='red')
plt.title('Applicant Income Distribution')
plt.show()

#Loan Amount Distribution
plt.figure(figsize=(8,5))
sns.histplot(df['LoanAmount'], kde=True, bins=30, color='green')
plt.title('Loan Amount Distribution')
plt.show()

#step 4. Bivariate
#Loan Status by Gender
plt.figure(figsize=(6,4))
sns.countplot(x='Gender', hue='Loan_Status', data=train, palette='coolwarm')
plt.title('Loan Status by Gender')
plt.show()

#Loan Status by Education
plt.figure(figsize=(6,4))
sns.countplot(x='Education', hue='Loan_Status', data=train, palette='Set2')
plt.title('Loan Status by Education')
plt.show()

#Income vs Loan Amount
plt.figure(figsize=(8,5))
sns.scatterplot(x=df1['ApplicantIncome'], y=df1['LoanAmount'], hue=df1['Loan_Status'])
plt.title('Applicant Income vs Loan Amount')
plt.show()

#step 5. Correlation Heatmap
# Excluding non-numeric columns from correlation calculation
correlation_matrix = df.select_dtypes(include=np.number).corr()
plt.figure(figsize=(10,8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, roc_curve, confusion_matrix
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def evaluate_model(y_test, y_pred, y_prob):
    """
    Evaluates the performance of a classification model and plots the ROC curve and confusion matrix.

    Parameters:
    y_test (array-like): True labels.
    y_pred (array-like): Predicted labels.
    y_prob (array-like): Predicted probabilities for the positive class.
    """

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob)
    mcc = matthews_corrcoef(y_test, y_pred)

    # Print evaluation metrics
    print("Model Evaluation Metrics:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"ROC AUC Score: {roc_auc:.4f}")
    print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")

    # Plot ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, linestyle='-', label=f'ROC Curve (area = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()

    # Plot Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

# Example Usage with synthetic data
# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a simple logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Get predictions and probabilities
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]  # Probability of positive class

# Evaluate the model
print("Example Model Evaluation:")
evaluate_model(y_test, y_pred, y_prob)